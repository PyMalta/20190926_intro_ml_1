{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:09.503316Z",
     "start_time": "2019-09-30T14:36:09.500811Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:09.507867Z",
     "start_time": "2019-09-30T14:36:09.505093Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 18,\n",
    "                            'lines.linewidth' : 3,\n",
    "                           'figure.figsize' : [15, 5],\n",
    "                           'lines.markersize': 10})\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous session\n",
    "We had a teaser of what to expect when doing Machine Learning. By now you should know:\n",
    "1. How to use `scikit-learn`\n",
    "    - **predictors**: `fit(X)`, `predict(X)`\n",
    "    - **transformers**: `fit(X)`, `transform(X)`, `fit_transform(X)`\n",
    "1. What metrics to use for Supervised classification learning    \n",
    "1. Be able to build your first `Decision Tree` binary classifier using the *DieTanic* dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "We have defined machine learning as the process of optimizing the cost function of the problem by tweaking the parameters of a model. We have defined a model as some map that uniquely maps input (features) to output (labels or target values). We've left this definition very vague, because there are many popular models used for machine learning. We have already looked at decision trees and now we will investigate how to choose the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:09.528280Z",
     "start_time": "2019-09-30T14:36:09.510286Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic = pd.read_csv('./00_data/titanic_X.csv')\n",
    "df_titanic_target = pd.read_csv('./00_data/titanic_y.csv', squeeze=True,header=None)\n",
    "\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:09.534081Z",
     "start_time": "2019-09-30T14:36:09.530044Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:09.539187Z",
     "start_time": "2019-09-30T14:36:09.535627Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(df_titanic)\n",
    "y = np.array(df_titanic_target)\n",
    "\n",
    "df_titanic['Survived'] = df_titanic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "1. Load **./00_data/dietanic.csv**\n",
    "1. Identify the feature matrix `X` and the labels `y`.\n",
    "2. Create training and testing dataset: `X_train, y_train, X_test, y_test` of test size 20% with seed 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Recap*: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:09.924569Z",
     "start_time": "2019-09-30T14:36:09.541346Z"
    }
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# train decision tree\n",
    "dtc = DecisionTreeClassifier(max_depth=2,)\n",
    "y_pred = dtc.fit(X,y).predict(X)\n",
    "\n",
    "# visual tree\n",
    "graphviz.Source(export_graphviz(dtc, \n",
    "                                out_file=None,\n",
    "                                feature_names=np.array(df_titanic.drop('Survived', axis=1).columns.tolist()),\n",
    "                                class_names=np.array(['0','1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By allowing for more branching, does this make our model better or worse?Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:09.931662Z",
     "start_time": "2019-09-30T14:36:09.926820Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('f1-Score : {}'.format(f1_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:10.248887Z",
     "start_time": "2019-09-30T14:36:09.933123Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = range(1,40)\n",
    "f1Score = []\n",
    "for md in max_depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=md)\n",
    "    y_pred = dtc.fit(X,y).predict(X)\n",
    "    f1Score.append(f1_score(y, y_pred))\n",
    "    \n",
    "plt.plot(max_depths, f1Score, label = 'f1-Score')\n",
    "plt.xlabel('Maximum Depth Hyperparameter')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach a conflict: the model keeps getting better and better the deeper we go with **max_depth** and beyond 18, we are suffering from _overfitting_. The model does not seem to improve and at 24 max_depth, the precision actually decreases as this now starts to follow noise in the data. To detect overfitting, we need to see how our model generalizes to new data. We can do this artificially by withholding part of our data set during the training step, and then using it to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:10.609604Z",
     "start_time": "2019-09-30T14:36:10.250961Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_titanic.drop(['Survived'], axis=1),\n",
    "                                                   df_titanic['Survived'],\n",
    "                                                   test_size = 0.1)\n",
    "f1Score_test = []\n",
    "for md in max_depths:\n",
    "    dtc = DecisionTreeClassifier(max_depth=md)\n",
    "    y_pred = dtc.fit(X_train,y_train).predict(X_test)\n",
    "    f1Score_test.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(max_depths, f1Score, label = 'f1-Score')\n",
    "plt.plot(max_depths, f1Score_test, '--', label = 'f1-Score test')\n",
    "plt.xlabel('Maximum Depth Hyperparameter')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing error confirms our suspicion. As the model becomes more complex, it improves up to a point, and then it loses generalizability. **The testing metrics actually start decreasing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "1. Train a decision tree classifier with hyperparameter **max_depth = 3** and random_state=55\n",
    "1. Evaluate the f1-score on the test data\n",
    "1. Train a decision tree classifier with hyperparameter **max_depth = 15**, random_state=55 and evaluate the f1-score\n",
    "1. Train a decision tree classifier with hyperparameter **max_depth = 40**, random_state=55 and evaluate the f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters: tuning and cross-validation\n",
    "\n",
    "DTC has multiple hyper-parameters:\n",
    "- `max_depth`,\n",
    "- `min_samples_split`\n",
    "- [DTC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "**Which is optimal?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:10.723807Z",
     "start_time": "2019-09-30T14:36:10.610815Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth = [5,10,15,20,25,30]\n",
    "min_samples_split = [25, 50, 75, 100, 125, 150, 175, 200]\n",
    "import itertools\n",
    "\n",
    "combns = [ii  for ii in itertools.product(max_depth, min_samples_split)] \n",
    "\n",
    "combns_x = [x[0] for x in combns]\n",
    "combns_y = [x[1] for x in combns]\n",
    "plt.scatter(combns_x, combns_y)\n",
    "plt.xlabel('max_depth')\n",
    "plt.xticks(max_depth)\n",
    "plt.ylabel('min_samples_split')\n",
    "plt.yticks(min_samples_split)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since changing hyperparameters changes the structure of the model, we should think of choosing hyperparameters as part of model selection. `Scikit-learn` provides a useful tool for comparing different hyperparameter values, `GridSearchCV`. There are two ideas behind `GridSearchCV`: first we will split up the data into a training and validation set (using a method called [k-folds](http://scikit-learn.org/stable/modules/cross_validation.html#k-fold)) and then we train and evaluate models with different hyperparameters selected from a grid of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:11.163319Z",
     "start_time": "2019-09-30T14:36:10.725449Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "grid = GridSearchCV(dtc,\n",
    "                   {'max_depth' : range(1,15),\n",
    "                   'min_samples_split': range(10, 110, 10)},\n",
    "                    cv=5,\n",
    "                    n_jobs=5,\n",
    "                    scoring='f1',\n",
    "                   verbose=1)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:11.167795Z",
     "start_time": "2019-09-30T14:36:11.164668Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:11.172068Z",
     "start_time": "2019-09-30T14:36:11.169192Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:11.178952Z",
     "start_time": "2019-09-30T14:36:11.174175Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:11.183870Z",
     "start_time": "2019-09-30T14:36:11.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "len(range(1,15)) * len(range(10, 110, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise 3:\n",
    "1. Find the optimal decision tree classifier using a parameter search of:\n",
    "    - **max_depth**: 5 to 25, increments of 5\n",
    "    - **min_samples_split**: \\[2, 6, 10 \\]\n",
    "    - **max_features**: *see documentation for this**\n",
    "    - **random_state**: set seed to be 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Caution*: Compounded growth of hyperparameter grid search\n",
    "\n",
    "Caution is advised when doing hyperparameter tuning using GridSearchCV. Each additional hyperparameter added to the grid search compoundedly grows the number of parameters to search. For example, consider 3 hyperparameters for decision tree, with respective search arrays\n",
    "\n",
    "|Hyperparameter| Search Array | Number of hyperparameters|\n",
    "|---|---|---|\n",
    "|max_depth |*\\[5, 15, 25, 30, 45, 55, 75, 100, 125, 150\\]* | 10 |\n",
    "|max_features |*\\['auto','sqrt','log'\\]* | 3 |\n",
    "|min_samples_leaf| *\\[25, 50, 75, 100, 125, 150, 175, 200 \\]* |8|\n",
    "\n",
    "Assume that we are doing 5-fold cv on the data,\n",
    "\n",
    "Total number of hyperparameters to be searched are: $ 10*3*8*5 = 1200 $\n",
    "\n",
    "If we add 2 more hyperparamters to *max_depth*, we now need to search through $6*3*8*5 = 1440$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:11.409148Z",
     "start_time": "2019-09-30T14:36:11.185524Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot3DSearchGrid(angle):\n",
    "    xx = [5, 15, 25, 30, 45, 55, 75, 100, 125, 150]\n",
    "    yy = ['auto','sqrt','log']\n",
    "    zz = [25, 50, 75, 100, 125, 150, 175, 200 ]\n",
    "\n",
    "    X, Y, Z = np.mgrid[range(len(xx)), range(len(yy)), range(25,225,25)]\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scat = ax.scatter(X, Y, Z, c=Z.flatten(), s=100)\n",
    "    plt.xticks(np.arange(len(xx)), xx)\n",
    "    ax.set_xlabel('max_depth', labelpad=30)\n",
    "    plt.yticks(np.arange(len(yy)), yy)\n",
    "    ax.set_ylabel('min_samples_leaf', labelpad=30)\n",
    "\n",
    "    ax.set_zlabel('max_features', labelpad=30)\n",
    "    ax.view_init(30, angle)\n",
    "    \n",
    "axes_angle = IntSlider(min=0, max=180, step=15, description='Angle') \n",
    "interact(plot3DSearchGrid, angle=axes_angle);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further look at Decision Trees and other Tree Based Models\n",
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T13:31:08.504710Z",
     "start_time": "2019-09-30T13:31:08.501254Z"
    }
   },
   "source": [
    "<img src =\"00_pics/random_forest.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T12:45:27.381413Z",
     "start_time": "2019-08-22T12:45:27.372285Z"
    }
   },
   "source": [
    "The performance of a single decision tree will be limited. Instead of relying on one tree, a better approach is to aggregate the predictions of multiple tree. On average, aggregation will perform better than a single predictor. You can envision the aggregation as mimicking the idea of \"wisdom of the crowd\". We call a tree based model that aggregates the predictions of multiple trees a __random forest__.\n",
    "\n",
    "In order for a random forest to be effective, the model needs a diverse collection of trees. There should be variations in the chosen thresholds for splitting and the number of nodes and branches. There is no point in aggregating the predicted results if all the trees are nearly identical and produce the same result. There is no \"wisdom of the crowd\" if everyone thinks alike. To achieve a diverse set of trees, we need to:\n",
    "\n",
    "1. Train each tree in the forest using a different training set.\n",
    "1. Only consider a subset of features when deciding how to split the nodes.\n",
    "\n",
    "For the first point, ideally we would generate a new training set for each tree. However, often times it's too difficult or expensive to collect more data; we have to make due with what we have. Bootstrapping is a general statistical technique to generate \"new\" data sets with a single set by random sampling with _replacement_. Sampling with replacement allows for a data point to be sampled more than once.\n",
    "\n",
    "Typically, when training the standard decision tree model, the algorithm will consider all features in deciding the node split. Considering only a subset of your features ensures that your trees do not resemble each other. If the algorithm had considered all features, a dominant feature would be continuously chosen for node splits.\n",
    "\n",
    "The hyperparameters available for random forests include those of decision tress with some additions.\n",
    "\n",
    "|Hyperparameter | Description |\n",
    "|---|---|\n",
    "|n_estimators|The number of trees in the forest|\n",
    "|n_jobs|The number of jobs to run in parallel when fitting and predicting|\n",
    "|warm_start|If set to `True`, reuse the trained tree from a prior fitting and just train the additional trees|\n",
    "\n",
    "\n",
    "Since the random forest is based on idea of bootstrapping and aggregating the results, it is referred to as a *bagging* ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a plot of the test set f1-Score as a function of the size of the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T14:36:12.366931Z",
     "start_time": "2019-09-30T14:36:11.410495Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# X, y = make_moons(n_samples=10000, noise=0.3, random_state=0)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "def rfc_f1Score(max_features,n_max):\n",
    "    rfc = RandomForestClassifier(max_features=max_features,\n",
    "                                max_depth=8,\n",
    "                                n_estimators=1,\n",
    "                                warm_start=True,\n",
    "                                random_state=0)\n",
    "    \n",
    "    f1Score = np.zeros(n_max)\n",
    "\n",
    "    for n in range(1, n_max):\n",
    "        rfc.set_params(n_estimators=n)\n",
    "        rfc.fit(X_train, y_train)\n",
    "        f1Score[n-1] = f1_score(y_test, rfc.predict(X_test))\n",
    "    \n",
    "    return f1Score\n",
    "\n",
    "f1 = rfc_f1Score(max_features='sqrt', n_max=100)\n",
    "plt.plot(f1[:-1], label = 'sqrt')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('f1-Score');\n",
    "# plt.legend();    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several takeaways from the results plotted above.\n",
    "\n",
    "1. There is sharp increase in metric performance when initially growing the forest.\n",
    "1. In general, as the number trees increase, performance increases.\n",
    "1. You _can_ overfit with a large number of trees but the model is robust to overfitting with the number of trees\n",
    "1. Note, the model was not tuned for hyperparameters like `max_depth` and `min_samples_split`.\n",
    "\n",
    "The initial increase in f1-Score can be attributed to a large increase in diverse trees when the forest is small. In other words, the additional trees will be very different from the current trees simply because the forest is small. The increase in tree diversity drives predictive power. As the forest grows, newer trees will not be significantly different from the current pool of trees as bootstrapping is no longer producing substantially diverse training sets to create very different looking trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4:\n",
    "1. Fit a Random Forest Classifier and evaluate the f1-score\n",
    "2. Compare to the default Decision Tree result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Trees\n",
    "<img src=\"00_pics/gradient_boosting.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting trees are another ensemble model; it is collection of tree models arranged in a sequence. Here, the model is built stage-wise; each additional tree aims to correct the previously built model's predictions. A model with $M$ trees is equal to \n",
    "\n",
    "$$ f_M(x_j) =  \\sum^M_m \\gamma_m h_m(x_j), $$\n",
    "\n",
    "where $h_m$ is a **weak learner** decision tree, a \"stump\" model with low depth that performs poorly on its own. The term **boosting** refers to the algorithm's ability to combine multiple weak learners to form a strong learner. $\\gamma_m$ is a factor that scales the contribution of a tree to the overall model. How are $h_m$ and $\\gamma_m$ chosen? The model is usually initialized with $h_0$ being equal to the mean of the training labels for regression or the majority class for classification. We also need to choose a loss function $L(y, f_m)$. For example, if the loss function is squared error, then\n",
    "\n",
    "$$ L_{SE}(y_j, f_m(x_j)) = (y_j-f_m(x_j))^2. $$ \n",
    "\n",
    "The steps for building our model with $M$ trees at each stage is\n",
    "\n",
    "1. Compute the **pseudo-residuals**, the derivative of the loss function with respect to the previous model $f_{m-1}$. The equation for the pseudo-residuals for a model at stage $m$ is\n",
    "$$ r_{jm} = - \\left[\\frac{\\partial L(y_j, f(x_j))}{\\partial f} \\right]_{f(x_j) = f_{m-1}(x_j)}. $$\n",
    "1. Train $h_m$ on the **pseudo-residuals** $r_{jm}$.\n",
    "1. Choose $\\gamma_m$ that minimizes $L(y_j, f_{m-1}(x_j) + \\gamma_m h_m(x_j)).$\n",
    "1. Form the improved model equal to\n",
    "$$ f_m(x_j) = f_{m-1}(x_j) + \\gamma_m h_m(x_j). $$\n",
    "1. Repeat until the model includes all $M$ trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the name gradient come from in gradient boosting trees? Adding a model is analogous to a single iteration in gradient descent. Gradient descent is a minimization algorithm that updates/improves the current answer by taking a step in the direction of the negative of the gradient of the function that is being minimized. The pseudo-residuals represent the direction of greatest reduction in prediction error. Since $h_m$ is trained on the direction of greatest descent of the loss, it will be an approximation of the improvement required for our model to fit the data more closely. $\\gamma_m$ is the step size we should take in the direction of greatest model improvement. Compare the equation above for an improved model with the equation of gradient descent applied to a function $C(\\beta_i)$;\n",
    "\n",
    "$$\\beta^{updated}_i = \\beta^{current}_i - \\gamma \\left(\\left.\\frac{\\partial C}{\\partial \\beta_i}\\right)\\right|_{\\beta_i=\\beta^{current}_i}.$$\n",
    "\n",
    "The concept of pseudo-residuals allows us to generalize gradient boosting trees for any loss function. In fact, when we use a square loss function like the squared error, the pseudo-residual is directly proportional to the residual, $y_j - f(x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting trees have a similar set of hyperparameters as random forests but with some key additions.\n",
    "\n",
    "|Hyperparameter | Description |\n",
    "|---|---|\n",
    "|learning_rate|Multiplicative factor of the tree's contribution to the model|\n",
    "|subsample|Fraction of the training data to use when fitting the trees|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate ranges form 0 to 1, with the modified equation being\n",
    "\n",
    "$$ f_m(x_j) = f_{m-1}(x_j) + \\nu \\gamma_m h_m(x_j), $$\n",
    "\n",
    "where $\\nu$ is the learning rate. The learning rate and subsampling fraction interact with each other, we will see this in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5:\n",
    "1. Fit a Gradient Tree Classifier, evaluate the f1-score\n",
    "2. Compare against the previous 2 models\n",
    "3. What can you do to improve your result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "So we now know how to:\n",
    "- Obtain the optimal hyperparameter for our model using GridSeachCV (hyperparameter tuning)\n",
    "- Modelling techniques:\n",
    "    - *Decistion Tree Classifier*\n",
    "    - Random Forest Classifiers, \n",
    "    - Gradient Boosted Tree Classifiers. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
