{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:52:01.229980Z",
     "start_time": "2019-08-23T08:52:01.227439Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "\n",
    "# pip install graphviz\n",
    "# conda install python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:52:01.235029Z",
     "start_time": "2019-08-23T08:52:01.231876Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 18,\n",
    "                            'lines.linewidth' : 3,\n",
    "                           'figure.figsize' : [15, 5],\n",
    "                           'lines.markersize': 10})\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean to learn?  Learning is a process where we take a series of observations and draw conclusions based on past experiences. For example, we can learn to recognize patterns in experiential data such as when I take the later bus, I'm late to work.  Machine Learning is when we teach a computer to do the same thing, namely find patterns in data.  The idea is that humans are really great at finding patterns, but relatively slow at looking through large amounts of data.  Computers need to be trained to find the patterns, but they can process data of the sort of we have (csv files, images, etc) incredibly fast.\n",
    "\n",
    "If we want to leverage machine learning, we need to teach computers to recognize patterns and leverage that ability to solve real world patterns.  Lets start with a really simple example.\n",
    "\n",
    "Say we have one dimensional data given by a single feature $X$ and a corresponding set of labels $y$.  We want to model this data, so we will create a relationship \\begin{equation} f(X) \\approx y .\\end{equation} This function $f$ will represent our model.  We will generate the data here by randomly choosing an exponent for a trend and adding some random noise.  Lets create the data and see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:52:01.397111Z",
     "start_time": "2019-08-23T08:52:01.237028Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1, 100)\n",
    "print(\"X: size {}\".format(len(X)))\n",
    "print(X)\n",
    "# exp = np.random.choice([2, 3])\n",
    "exp = 2\n",
    "y = X**exp + np.random.randn(X.shape[0])/10\n",
    "print(\"\")\n",
    "print(\"y: size {}\".format(len(y)))\n",
    "print(y)\n",
    "\n",
    "plt.plot(X, y, '.');\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate the predictive relationship by using one of the simplest possible methods, fitting a line to the data $$y=mx+c$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:52:03.461088Z",
     "start_time": "2019-08-23T08:52:03.287058Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def plotBestLine(m,c):\n",
    "    plt.plot(X, y, '.', label='data');\n",
    "    plt.plot(X, m*X + c, label = 'Model: {}x+{}'.format(m,c))\n",
    "    plt.legend();\n",
    "\n",
    "slider_m = FloatSlider(value=1, min=0, max=2, step=0.02, description='Gradient (m)')    \n",
    "slider_c = FloatSlider(value=0, min=-0.5, max=0.5, step=0.02, description='Intercept (c)')    \n",
    "interact(plotBestLine, m=slider_m, c=slider_c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:17.094986Z",
     "start_time": "2019-08-23T08:51:16.779090Z"
    }
   },
   "outputs": [],
   "source": [
    "p = np.polyfit(X, y, 1)\n",
    "z = np.poly1d(p)\n",
    "plt.plot(X, y, '.')\n",
    "plt.plot(X, z(X), label=r\"Model: ${:.2f}x + {:.2f}$\".format(*p))\n",
    "plt.plot(X, X**exp, label=r'Truth: $x^{}$'.format(exp))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a model for this data, learned by the computer, namely given an $X$ value (or a bunch of values), we can predict the output.  This example opens up many questions:\n",
    "\n",
    "1. How good is the model?\n",
    "2. Can we add flexibility to the model?\n",
    "3. Is the model generalizable?\n",
    "4. What does this model teach us about the data?\n",
    "\n",
    "Lets start with question 4, which in many ways is the most important question.  For this simple model we can see that the $y$ vector of labels has a positive correlation with the features $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we do Machine Learning?\n",
    "Normally the goal of machine learning is two-fold\n",
    "\n",
    "1. To understand the data we already have\n",
    "2. Use this understanding to make predictions about unlabeled data\n",
    "\n",
    "Machine Learning falls into two classes, \n",
    "- **supervised** learning : *learn a predictive relationship between **features** of our data and some sort of output **target** label* \n",
    "- **unsupervised** learning : *find trends in our features without using any target labels*\n",
    "\n",
    "A human example of supervised learning would be borrowing books from a library on mathematics and geography. By reading different books belonging to each topic, we learn what symbols, images, and words are associated with math, and which are associated with geography. \n",
    "\n",
    "A similar unsupervised task would be to borrow many books without knowing their subject. We can see some books contain similar images (maps) and some books contain similar symbols (e.g. the Greek letters $\\Sigma$ and $\\pi$). We say the books containing maps are similar and that they are different from the books containing Greek letters. Crucially, _we do not know what the books are about, only that they are similar or different_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine learning\n",
    "Formally, the supervised machine problem can be stated as:\n",
    "- given a matrix $X$,\n",
    "of dimensions $n \\times p$\n",
    "- create a predictive relationship (or function) $f(X)$ where \\begin{equation} f(X) \\approx y \\end{equation} \n",
    "    - $y$ is a vector of dimension $n$ called the **labels**.\n",
    "    - $X$ is referred to as the **feature matrix** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:17.561068Z",
     "start_time": "2019-08-23T08:51:17.096381Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "dt_cali = fetch_california_housing()\n",
    "df_cali = pd.DataFrame(dt_cali.data, columns=dt_cali.feature_names)\n",
    "df_target = [1 if ii < 2 else 0  for ii in dt_cali['target'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:17.589868Z",
     "start_time": "2019-08-23T08:51:17.563974Z"
    }
   },
   "outputs": [],
   "source": [
    "numRows = 20\n",
    "df_sup = df_cali.head(numRows)\n",
    "df_sup['-'] = '--------'\n",
    "df_sup['MedianHouseValue'] = dt_cali['target'][:numRows]\n",
    "# df_sup['AffordableHouse'] = df_target[:numRows]\n",
    "print('Supervised Learning Dataset:')\n",
    "df_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general goal of supervised learning is to then apply this model to unlabeled data where can build a feature matrix representative of the original.  This allows us to make predictions! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:17.604245Z",
     "start_time": "2019-08-23T08:51:17.591979Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sup = df_cali.head(numRows+5)[-5:]\n",
    "df_sup['-'] = '--------'\n",
    "df_sup['MedianHouseValue'] = '?'\n",
    "# df_sup['AffordableHouse'] = '?'\n",
    "df_sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:17.619358Z",
     "start_time": "2019-08-23T08:51:17.605750Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Unsupervised Learning Dataset:')\n",
    "df_cali.head(numRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Difficulties\n",
    "Machine learning is just a tool and is not a 'one-fits-all' solution.\n",
    "\n",
    "Models can be heavily biased and thus not flexible enough to handle generalization.  Let us plot our original function over a larger range and use the model from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:17.998007Z",
     "start_time": "2019-08-23T08:51:17.620839Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "X = np.linspace(0, 1, 100)\n",
    "y = X**exp + np.random.randn(X.shape[0])/10\n",
    "p = np.polyfit(X, y, 1)\n",
    "z = np.poly1d(p)\n",
    "ax[0].plot(X, y, '.')\n",
    "ax[0].plot(X, z(X), '-', linewidth=4, label= \"Model: ${:.2f}x + {:.2f}$\".format(*p))\n",
    "ax[0].plot(X, X**exp,'-', label=r'Truth: $x^{}$'.format(exp))\n",
    "ax[0].legend();\n",
    "ax[0].set_title('Original Model');\n",
    "\n",
    "X = np.linspace(0, 2, 100)\n",
    "y = X**exp + np.random.randn(X.shape[0])/10\n",
    "ax[1].plot(X, y, '.')\n",
    "ax[1].plot(X, z(X), 'g', linewidth=4, label= \"Model: ${:.2f}x + {:.2f}$\".format(*p))\n",
    "ax[1].plot(X, X**exp,'-', color='r', label=r'Truth: $x^{}$'.format(exp))\n",
    "ax[1].legend();\n",
    "ax[1].set_title('Extending $x$-axis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model works fairly well for the range over which initially considered our data, but we can see it will not generalize well to features outside the of the range we considered.  This is a general problem; we should be careful that our training data contains a well sampled distribution over which we expect to make predictions (or we have some prior knowledge that tells us we should be able to extrapolate beyond the domain of our training data).  Machine learning finds patterns in data that it's already seen, and it can't always make good predictions on data it hasn't. \n",
    "\n",
    "Lets try to fix this by adding more parameters to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:18.213618Z",
     "start_time": "2019-08-23T08:51:17.999345Z"
    }
   },
   "outputs": [],
   "source": [
    "p = np.polyfit(X, y, 15)\n",
    "z = np.poly1d(p)\n",
    "plt.figure(figsize=[14, 6])\n",
    "plt.plot(X, z(X), 'g', label=r\"${:.2f}x^{{15}} + {:.2f}x^{{14}} + ... + {:.2f}$\".format(*p[[0, 1, -1]]))\n",
    "plt.plot(X, y,'.', label=r'$x^{}$'.format(exp))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow looks like a perfect fit!  Maybe too good?  It looks like the model is fitting little wiggles in the data which we know are not real (the actual data is derived from a simple exponent).  Lets try to generalize again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:18.386165Z",
     "start_time": "2019-08-23T08:51:18.215112Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 2.5, 100)\n",
    "y = X**exp + np.random.randn(X.shape[0])/10\n",
    "plt.plot(X, z(X), 'g', linewidth=4, label=r\"model\")\n",
    "plt.plot(X, y,'.', label=r'$x^{}$'.format(exp))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow again!  That is pretty bad.  This is an example of overfitting, where we have allowed the model too much flexibility and it has fit the noise in the data which is not generalizable.\n",
    "\n",
    "We will learn more how to combat these issues, but the point is that we need to be careful when choose the model we want to use and the **hyperparameters** (in this case order of the polynomial) for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-learn` is the most popular Python package for machine learning. It has a plethora of machine learning models and a nice and intuitive interface. It makes creating complicated machine learning workflows very easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models as classes\n",
    "\n",
    "`Scikit-learn` relies heavily on object-oriented programming principles. It implements machine learning algorithms as classes and users create objects from these \"recipes\". For example, `LinearRegression` is a class representing the Linear Regression model. To create a `lr` object, we simply create an instance of the class. In Python, the convention is that class names use CamelCase, the first letter of each word is capitalized. `Scikit-learn` adopts the convention, making it easy to distinguish what is a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:18.480528Z",
     "start_time": "2019-08-23T08:51:18.387417Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we set `fit_intercept=False`. Here, `fit_intercept` is a **hyperparameter** of the Linear Regression model. Hyperparameters are model parameters that govern the learning process. In terms of hierarchy, they reside \"above\" the regular model parameters. They control what values the model parameters are equal to after undergoing training. They can be easily identified as they are the parameters that are set _prior_ to learning. In `scikit-learn`, hyperparameters are set when creating an instance of the class. The default values that `scikit-learn` uses are *usually* a good set of initial values but this is not always the case. It is important to understand the hyperparameters available and how they affect model performance.\n",
    "\n",
    "`Scikit-learn` refers to machine learning algorithms as **estimators**. There are three different types of estimators: \n",
    "1. Classifiers, \n",
    "1. Regressors, and \n",
    "1. Transformers. \n",
    "\n",
    "Programmatically, `scikit-learn` has a base class called `BaseEstimator` that all estimators inherit. The models inherit an additional class, either `RegressorMixin`, `ClassifierMixin`, and `TransformerMixin`. The inheritance of the second class determines what type of estimator the model represents. We'll divide the estimators into two groups based up on their interface. These two groups are **predictors** and **transformers**.\n",
    "\n",
    "Full information is available in the [documentation](https://scikit-learn.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors: Classifiers and Regressors\n",
    "\n",
    "As the name suggests, predictors are models that make predictions. There are two main methods.\n",
    "\n",
    "* `fit(X, y)`: trains/fit the object to the feature matrix $X$ and label vector $y$.\n",
    "* `predict(X)`: makes predictions on the passed data set $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:18.497161Z",
     "start_time": "2019-08-23T08:51:18.481705Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create model and train/fit\n",
    "model = LinearRegression()\n",
    "model.fit(df_cali, df_target)\n",
    "\n",
    "# predict label values on X\n",
    "y_pred = model.predict(df_cali)\n",
    "\n",
    "print(\"Prediction : {}\".format(y_pred))\n",
    "print(\"     Shape of the prediction array: {}\".format(y_pred.shape))\n",
    "print(\"     Shape of the training set: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the output of `predict(X)` is a NumPy array of one dimension. The array has the same size as the number of rows of the data that was passed to the `predict` method. \n",
    "\n",
    "Since we are using linear regression and our data has eight features, our model is\n",
    "\n",
    "$$ y(X) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\beta_5 x_5 + \\beta_6 x_6 + \\beta_7 x_7 + \\beta_8 x_8 + \\beta_0. $$\n",
    "\n",
    "The coefficients are stored in the fitted model as an object's attribute. `Scikit-learn` adopts a convention where all attributes that are determined/calculated _after_ fitting end in an underscore. The model coefficients and intercept are retrieved using the `coefs_` and the `intercept_` attributes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:18.503758Z",
     "start_time": "2019-08-23T08:51:18.498925Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"β_0: {}\".format(model.intercept_))\n",
    "for i in range(8):\n",
    "    print(\"β_{}: {}\".format(i+1, model.coef_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T09:31:11.900465Z",
     "start_time": "2019-08-20T09:31:11.895710Z"
    }
   },
   "source": [
    "If we wanted to know how well the model performs making predictions with a data set, we can use the `score(X, y)` method. It works by\n",
    "\n",
    "1. Internally running `predict(X)` to produce predicted values.\n",
    "1. Using the predicted values to evaluate the model compared to the true label values that were passed to the method.\n",
    "\n",
    "The evaluation equation varies depending if the model is a regressor or classifier. For regression, it is the $R^2$ value while for classification, it is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:18.514463Z",
     "start_time": "2019-08-23T08:51:18.505973Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"R^2: {:g}\".format(model.score(df_cali, df_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a rather simple model, linear regression. What if we wanted to use a more complicated model? All we need to do is an easy substitution; there is minimum code rewrite as the models have the same interface. Of course, different models have different hyperparameters so we need to be careful when swapping out algorithms. Let's use a more complicated model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.569420Z",
     "start_time": "2019-08-23T08:51:18.516449Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# create model and train/fit\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(df_cali, df_target)\n",
    "\n",
    "# predict label values on X\n",
    "y_pred = model.predict(df_cali)\n",
    "\n",
    "print(y_pred)\n",
    "print(\"R^2: {:g}\".format(model.score(df_cali, df_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are models that process and transform a data set. These transformers are very useful because rarely is our data in a form to feed directly to a machine learning model for both training and predicting. All transformers have the same interface:\n",
    "\n",
    "* `fit(X)`: trains/fits the object to the feature matrix $X$.\n",
    "* `transform(X)`: applies the transformation on $X$ using any parameters learned\n",
    "* `fit_transform(X)`: applies both `fit(X)` and then `transform(X)`.\n",
    "\n",
    "Let's demonstrate transformers with `StandardScaler`, which scales each feature to have zero mean and unit variance. The transformed feature $x'_i$ is equal to\n",
    "\\begin{equation} x'_i = \\frac{x_i - \\mu_i}{\\sigma_i} \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.588972Z",
     "start_time": "2019-08-23T08:51:19.570909Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create and fit scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_cali)\n",
    "\n",
    "# scale data set\n",
    "Xt = scaler.transform(df_cali)\n",
    "\n",
    "# create data frame with results\n",
    "stats = np.vstack((df_cali.mean(axis=0), df_cali.var(axis=0), Xt.mean(axis=0), Xt.var(axis=0))).T\n",
    "feature_names = df_cali.columns\n",
    "columns = ['unscaled mean', 'unscaled variance', 'scaled mean', 'scaled variance']\n",
    "\n",
    "pd.DataFrame(stats, index=feature_names, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frame shows how our features have wildly different scales; the average population is over 1000 but the average room is slightly over 5. Now, our features each have zero mean and a variance of one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning for a Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset - Titanic Likelihood of Survival \n",
    "For this notebook, we will use the [Titanic Likelihood of Survival](https://www.kaggle.com/c/titanic/data) data. The data set contains the likelihood of survival of a person on board of the titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.603165Z",
     "start_time": "2019-08-23T08:51:19.590568Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic = pd.read_csv('./00_data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.616795Z",
     "start_time": "2019-08-23T08:51:19.604855Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_titanic.shape)\n",
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data dictionary**\n",
    "\n",
    "| Variable\t| Definition|\tKey |\n",
    "|---|---|---|\n",
    "|survived |\tSurvival |\t0 = No, 1 = Yes|\n",
    "|pclass |\tTicket class |\t1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|sex \t|Sex \t||\n",
    "|Age |\tAge in years \t||\n",
    "|sibsp \t|# of siblings / spouses aboard the Titanic \t||\n",
    "|parch \t|# of parents / children aboard the Titanic \t||\n",
    "|ticket \t|Ticket number \t||\n",
    "|fare \t|Passenger fare \t||\n",
    "|cabin |\tCabin number \t||\n",
    "|embarked |\tPort of Embarkation |\tC = Cherbourg, Q = Queenstown, S = Southampton|\n",
    "\n",
    "**Variable Notes**\n",
    "- *pclass* : A proxy for socio-economic status\n",
    "    - 1st = Upper\n",
    "    - 2nd = Middle\n",
    "    - 3rd = Lower\n",
    "- *age* : Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "- *sibsp* : The dataset defines family relations in this way...\n",
    "    - Sibling = brother, sister, stepbrother, stepsister\n",
    "    - Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "- *parch* : The dataset defines family relations in this way...\n",
    "    - Parent = mother, father\n",
    "    - Child = daughter, son, stepdaughter, stepson\n",
    "    - Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics of supervised learning\n",
    "\n",
    "For supervised ML, our model receives a vector of **features**, $X$, and maps it to some predicted label, $y$. In order to train our model, we will need many **observations** (i.e. measurements) and their associated labels. We can assemble these observations into a matrix.\n",
    "$$ f(X_{ij}) \\approx y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.630072Z",
     "start_time": "2019-08-23T08:51:19.618521Z"
    }
   },
   "outputs": [],
   "source": [
    "df_titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T09:50:12.851553Z",
     "start_time": "2019-08-20T09:50:12.846378Z"
    }
   },
   "source": [
    "In the above dataframe, each column is a feature (i.e. a variable) and each row is an observation (i.e. a measurement). Said another way, things like **pclass**, **sex** and **age** are features. The column **survived** is the target labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Classification Problems\n",
    "In classification, we predict categorical labels. In regression, we predict quantitative/numerical labels. The critical difference is that we can't take a difference between the predicted and actual category in classification, while we can take a difference between the predicted and actual numerical values in regression. Because of these differences between regression and classification, we use different metrics to evaluate machine learning models trained for classification.\n",
    "\n",
    "We are trying to determine the model $f$ that can best describes the relationship\n",
    "\n",
    "$$ y_j = f(X_j). $$\n",
    "\n",
    "For classification, $y_j$ can only take a finite set of values. If there are only two such values, we are dealing with **binary** classification. Examples of binary classification are predicting whether it will rain or not and whether someone will default on their loan. If we have more than two classes, we have a **multiclass** problem. For example, image classification is usually multiclass as we are trying to identify an image among a set of values, e.g., a person, a road sign, a car, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T10:00:12.380074Z",
     "start_time": "2019-08-20T10:00:12.374107Z"
    }
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "A natural choice for a metric for classification is accuracy. Accuracy is equal to the number of observations you correctly classified over all observations. For example, if your model properly identified 77 out of 100 images, you have an accuracy of 77%. Accuracy is an easy metric to both understand and calculate. Mathematically, it is simply\n",
    "\n",
    "$$ \\frac{\\text{number of correct observations}}{\\text{number of observations}}.$$\n",
    "\n",
    "However, accuracy may not always be a good metric. \n",
    "\n",
    "Consider the case of disease detection where only 10% of the observations have the disease. A \"stupid\" classifier that always predicts the majority class. For example, consider 100 observations with 10 rows as being infected. If the model predicts all as \"No disease\",\n",
    "\n",
    "\\begin{align}\n",
    "\\text{True Label} &: \\left[0,0,0,0,0,0,0,0,0,0,0,...,0,1,1,1,1,1,1,1,1,1,1\\right]\\\\\n",
    "\\text{Predicted } &: \\left[0,0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0,0\\right] \\\\\n",
    "\\\\\n",
    "\\frac{\\text{number of correct observations}}{\\text{number of observations}}\n",
    "&= \\frac{90}{100} = 90\\%\n",
    "\\end{align}\n",
    "\n",
    "*Realistically, this model has learning \"nothing\"* as it fails to identify any person with the disease. \n",
    "\n",
    "We need a metric that will tell us how well our model performs for a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, f1-Score and Confustion Matrix\n",
    "####  Precision & Recall\n",
    "For the example of disease detection, we are more interested in determining our model's performance with regards to the class representing having the disease. Let's call this class  **positive** and not having the disease as **negative**. \n",
    "\n",
    "We want to know what fraction of all positive predictions were correct and what fraction of positive observations did we identify. The two metrics that describe these values are **precision** and **recall**. \n",
    "- Precision is the fraction of true positives over all positive predictions. It is a measure of how \"precise\" our model was with regards to labeling observations as positive. \n",
    "- Recall is the fraction of true positives over all positive observations. It is a measure of our model's ability to \"catch\" and properly label observations that are positive.\n",
    "\n",
    "To summarize these 2 metrics, we construct a **confusion matrix**. This is a table summarizing the performance of the model by enumerating true and false positives and the true and false negatives.\n",
    "\n",
    "\n",
    "|             *       | Positive Observation (1)   | Negative Observation (0)   |\n",
    "|---------------------|:------------------------:|:-----------------------:|\n",
    "| Positive Prediction (1) |     True Positive (TP)   | False Positive (FP)     |\n",
    "| Negative Prediction (0) | False Negative (FN)      |     True Negative (TN)  |\n",
    "\n",
    "\n",
    "\n",
    "Given the definitions used earlier, the equation for precision and recall are\n",
    "\n",
    "$$ \\text{precision} = \\frac{\\text{TP}}{TP + FP}$$\n",
    "and\n",
    "$$ \\text{recall} = \\frac{\\text{TP}}{TP + FN}. $$\n",
    "\n",
    "Note, the difference between the metrics is their denominator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our disease detection example, if our data had 10 observations as true disease, \n",
    "\n",
    "\\begin{align}\n",
    "\\text{True Label} &: \\left[1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1\\right] \\\\\n",
    "\\text{Predicted}  & :\\left[1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0\\right] \\\\\n",
    "\\\\\n",
    "\\text{12 observations were marked as infected} & \\\\\n",
    "\\text{precision} & = \\frac{\\text{TP}}{TP + FP} = \\frac{8}{8+4} = 0.667\\% \\\\\n",
    "\\\\\n",
    "\\text{8 were correctly identified or \"recalled\"} &\\\\\n",
    "\\text{recall} &= \\frac{\\text{TP}}{TP + FN} = \\frac{8}{8+2} = 0.80\\%\n",
    "\\end{align}\n",
    "\n",
    "If we had used the previous \"simple\" model that predicts the all as 0 class, the recall would be 0 and our precision would be undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more important, precision or recall? The answer depends on the specifics of the problem. Having a model that prioritizes less false positives will have a higher precision and a model that aims to reduce the number of false negatives will have a higher recall. You must decide whether your model should prioritize reducing false positives or false negatives. It is often helpful to consider the cost, whether financial, societal, etc., of your model making false positives and false negatives.\n",
    "\n",
    "**Questions**\n",
    "* For disease detection, is it better to have a higher precision or recall?\n",
    "****\n",
    "* Does our answer change if we need to have diagnosed patients undergo invasive and risky procedures?\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T10:01:27.571761Z",
     "start_time": "2019-08-20T10:01:27.567758Z"
    }
   },
   "source": [
    "#### f1-Score\n",
    "In addition to precision and recall, there is that f1-score which is the harmonic mean of precision and recall. It is a nice metric to use when we don't have a preference over precision and recall. \n",
    "$$ \\text{f1-score} = 2\\times\\frac{\\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}} $$\n",
    "\n",
    "#### Classification Report\n",
    "We can easily calculate all these metrics using the `sklearn.metrics` module and to summarise all metrics, we will use the function `metrics.classification_report`. This will calculate the metrics for both scenarios of what class is considered positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.645217Z",
     "start_time": "2019-08-23T08:51:19.633170Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# generate our results\n",
    "y_pred = np.zeros(100, dtype=np.int32)\n",
    "y_pred[:12] = 1\n",
    "y = np.zeros(100)\n",
    "y[:8] = 1\n",
    "y[-2:] = 1\n",
    "\n",
    "print('True Labels : ')\n",
    "print([int(ii) for ii in y])\n",
    "print('\\nPredicted Labels :')\n",
    "print([int(ii) for ii in y_pred])\n",
    "print(\"\\nPrecision: {:g}\".format(metrics.precision_score(y, y_pred)))\n",
    "print(\"Recall: {:g}\".format(metrics.recall_score(y, y_pred)))\n",
    "print(\"Classification Report:\")\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Classification Models\n",
    "Some classification models do not directly predict a class for an observation but instead reports a probability. For example, it might predict that there's a 75% chance the observation is positive. For the preceding example, should we assign a positive or negative label? The natural choice is to assign the observation as positive since the predicted probability is greater than 50%. However, we don't have to stick to 50%; we can adjust our **threshold** and only classify observations as positive if our models predicts a greater than 90% probability. By increasing the threshold, we will make our model only make positive predictions when it is very certain and confident. Conversely, if we lower our threshold, our model will more liberally assign positive labels. Adjusting threshold affects the models precision and recall. \n",
    "\n",
    "As we started to see earlier, there is tradeoff between precision and recall that becomes more apparent with probabilistic models. Let's explore and visualize the tradeoff between precision and recall. We'll generate some data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.652241Z",
     "start_time": "2019-08-23T08:51:19.646925Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "np.random.seed(0)\n",
    "y_proba = np.linspace(0, 1, 1000)\n",
    "y_pred = (y_proba > 0.5).astype(np.int32)\n",
    "y = np.random.binomial(1, y_proba)\n",
    "\n",
    "print(\"accuracy: {}\".format(metrics.accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:19.854003Z",
     "start_time": "2019-08-23T08:51:19.653812Z"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, threshold = metrics.precision_recall_curve(y, y_proba)\n",
    "f1_score = 2*precision*recall/(precision + recall)\n",
    "threshold = np.hstack((0, threshold))\n",
    "\n",
    "plt.plot(threshold, precision)\n",
    "plt.plot(threshold, recall)\n",
    "plt.plot(threshold, f1_score)\n",
    "plt.xlabel('threshold')\n",
    "plt.legend(['precision', 'recall', '$F_1$']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, we see how increasing the threshold led to higher precision but lower recall. The threshold that yielded the largest $F_1$ score was about 0.36. Any probabilistic model can achieve any arbitrary level of precision and recall by adjusting the threshold. As such, when comparing the performance of probabilistic classifiers, we need a single metric that is not dependent on threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the curve\n",
    "The precision-recall curve illustrates the tradeoff for a particular classifier. While there will always be a tradeoff between these two metrics, ideally the tradeoff should not be severe. In other words, the model should not sacrifice a large amount of precision to slightly improve recall. We can visualize the degree of the tradeoff by plotting what is known as a precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:20.001214Z",
     "start_time": "2019-08-23T08:51:19.855299Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T10:02:29.415011Z",
     "start_time": "2019-08-20T10:02:29.409750Z"
    }
   },
   "source": [
    "We want a model that has less tradeoff between precision and recall, resulting in a curve with less of a drop with increasing recall. Geometrically, it is better to have a model with a larger area under the curve, **AUC**, of its precision-recall plot. In `scikit-learn`, the AUC can be calculated using the `metrics.auc` function. In addition to **AUC**, there is the **ROC-AUC** metric which is based on the receiver-operator curve (ROC). The ROC plots the true positive rate against the false negative rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T08:51:20.010655Z",
     "start_time": "2019-08-23T08:51:20.002907Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Precision-Recall AUC: {}\".format(metrics.auc(recall, precision)))\n",
    "print(\"Receiver-Operator AUC: {}\".format(metrics.roc_auc_score(y, y_proba)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, the resulting model had similar values for AUC and ROC. In general, if your data is imbalanced (more observation of the negative class) or if you care more about false positives you should rely on AUC of the precision-recall curve. Note, the number of true negatives are not factored in calculating either precision or recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "1. What is ML\n",
    "1. How to use `scikit-learn`\n",
    "    - **predictors**: `fit(X)`, `predict(X)`\n",
    "    - **transformers**: `fit(X)`, `transform(X)`,`fit_transform(X)`\n",
    "1. What metrics to use for Supervised classification learning    \n",
    "    - Accuracy,\n",
    "    - Precision, Recall, f1-score\n",
    "    - Confusion Matrix/Classification Report\n",
    "    - Area under curve (AUC)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbclean": true,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "166px",
    "width": "288px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
